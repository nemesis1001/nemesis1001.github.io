---
layout: post
title:  "[AWS]GatewayAPI / KinesisStream, Firehose / S3 이론"
subtitle:   "[AWS]GatewayAPI / KinesisStream, Firehose / S3 이론"
description : "AWS에서 Spark를 활용한 Pipeline 구축하기"
keywords : "AWS, Gatewaypit, Kinesis, KinesisStream, KinesisFirehose, S3"
categories: data
tags: fcaws
comments: true
---

# Spark을 활용한 데이터 파이프라인 만들기_2주차_이론


> ### 목표
> S3를 이해하자  
> Kinesis를 이해하자  
> 온프레미스와 클라우드상에서 데이터 수집의 차이를 알아보자  
> AWS 패키지들을 이용한 데이터 수집  


### 큰 흐름
- 데이터 생성
- 데이터 수집: Kinesis Streams, Kinesis Firehose, Gateway, Lambda function
- 데이터 수집 & 전처리: Glue, S3
- 데이터 전처리: EMR
- 데이터 분석, 시각화: Zeppelin, Athena, Tableau, Superset


### S3
- 무제한의 저장용량
- 요청이 많아도 Kinesis Stream의 shard 조정으로 확장 처리 가능
- 반 정형화 타입인 json으로 저장
- **버킷** : 모든 객체는 어떤 버킷에 포함된다. 윈도우에서 root 폴더라고 이해하자. 버킷은 웹서버 구성도 가능하고 생각보다 많은 것이 가능함
- **객체** : S3에 저장되는 기본 개체
	- 개체 = 객체 데이터 + 메타 데이터
- **키** : 버킷 내에서 객체를 구별하는 식별자로 모든 객체는 정확히 하나의 키를 가진다.


### Api Gateway
- 쉽게 말해서 모바일, 웹 애플리케이션에서 AWS 서비스들에 접근할 수 있는 일관된 Restful Api를 지원한다. AWS 솔루션을 쓰겠다? Api Gateway가 커넥터 역할을 하는듯 하다.


### Kinesis(= AWS의 Kafka)
- [카프카 설명 바로가기](https://twowinsh87.github.io/etc/2018/08/02/etc-kafka-1/)
- shard = broker
	- 압축된 800mb 정도는 shard 하나면 충분하다.


### Firehose
- Serverless
	- 실시간 스트리밍 데이터를 여러 곳에 전달해주는 역할을 한다
	- 실습에서는 이 것을 사용해서 S3에 저장하는 것을 해본다
	- **Spark Streaming, Kinesis Streaming (즉 스트리밍 형태)으로 들어오는 데이터를 S3, Redshift, Elasticsearch, Splunk 등에 전송을 위한 관리형 서비스다**


> 가장 이해하기 어려운 부분이었음  
> 우리의 프로젝트와 비교해서 설명하면  
> spark streaming -> 나름 리얼리얼 데이터를 받아서 처리해주는 부분에 작성한 프로세스를 이 Firehose가 한다고 보면 된다.
> 아래의 그림을 보면 이해가 될 듯  

<img src = "https://github.com/twowinsh87/twowinsh87.github.io/blob/master/assets/fastcamp_awsclass_img/KinesisStreamToKinesisFirehose.png?raw=true">


>**용어 알기**  
>**메타데이터** : 데이터에 대한 데이터. 즉, 데이터에 관한 구조화된 데이터로 다른데이터를 설명해주는 데이터 이다.

** 수업 내용의 일부를 정리한 것으로, 퍼가는 것을 지양해주세요 **
